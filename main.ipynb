{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (1158, 17)\n",
      "   survived deck embarked  pclass  embark_town     sex  adult_male    who  \\\n",
      "0         0  NaN        S     3.0  Southampton    male        True    man   \n",
      "1         1    C        C     1.0    Cherbourg  female       False  woman   \n",
      "2         1  NaN        S     3.0  Southampton  female       False  woman   \n",
      "3         1    C        S     NaN  Southampton  female       False  woman   \n",
      "4         0  NaN        S     3.0  Southampton    male        True    man   \n",
      "\n",
      "    wspd  lfwa  class      tprc  sibsp   age  alone     fare  parch  \n",
      "0   22.0  44.0  Third   23.7500    1.0  22.0  False   7.2500    0.0  \n",
      "1   38.0  76.0  First  215.8499    1.0  38.0  False  71.2833    0.0  \n",
      "2   26.0  52.0  Third   25.7750    0.0  26.0   True   7.9250    0.0  \n",
      "3   45.0  70.0  First  161.3000    NaN  35.0  False  53.1000    0.0  \n",
      "4  235.0  70.0  Third   26.1500    0.0  35.0   True   8.0500    NaN  \n",
      "After imputation: (1158, 17)\n",
      "After removing duplicates: (832, 17)\n",
      "After normalization:\n",
      "   survived deck embarked    pclass  embark_town     sex  adult_male    who  \\\n",
      "0       0.0  NaN        S  1.000000  Southampton    male        True    man   \n",
      "1       1.0    C        C  0.000000    Cherbourg  female       False  woman   \n",
      "2       1.0  NaN        S  1.000000  Southampton  female       False  woman   \n",
      "3       1.0    C        S  0.647954  Southampton  female       False  woman   \n",
      "4       0.0  NaN        S  1.000000  Southampton    male        True    man   \n",
      "\n",
      "       wspd      lfwa  class      tprc     sibsp       age  alone      fare  \\\n",
      "0  0.080390  0.271174  Third  0.014151  0.125000  0.293286  False  0.014151   \n",
      "1  0.140693  0.472229  First  0.139136  0.125000  0.510737  False  0.139136   \n",
      "2  0.095466  0.321438  Third  0.015469  0.000000  0.347649   True  0.015469   \n",
      "3  0.167075  0.434531  First  0.103644  0.064176  0.469965  False  0.103644   \n",
      "4  0.883164  0.434531  Third  0.015713  0.000000  0.469965   True  0.015713   \n",
      "\n",
      "      parch  \n",
      "0  0.000000  \n",
      "1  0.000000  \n",
      "2  0.000000  \n",
      "3  0.000000  \n",
      "4  0.060141  \n",
      "After removing redundant features: (832, 16)\n",
      "Training model...\n",
      "Accuracy: 0.8\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.77      0.71        13\n",
      "         1.0       0.88      0.81      0.85        27\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.77      0.79      0.78        40\n",
      "weighted avg       0.81      0.80      0.80        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Load dataset\n",
    "messy_data = pd.read_csv('messy_data.csv')\n",
    "print(\"Data loaded. Shape:\", messy_data.shape)\n",
    "print(messy_data.head())\n",
    "\n",
    "# 2. Impute missing values\n",
    "def impute_missing_values(data, strategy='mean'):\n",
    "    if strategy == 'mean':\n",
    "        return data.fillna(data.mean(numeric_only=True))\n",
    "    elif strategy == 'median':\n",
    "        return data.fillna(data.median(numeric_only=True))\n",
    "    elif strategy == 'mode':\n",
    "        return data.fillna(data.mode().iloc[0])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Choose 'mean', 'median', or 'mode'.\")\n",
    "\n",
    "clean_data = impute_missing_values(messy_data, strategy='mean')\n",
    "print(\"After imputation:\", clean_data.shape)\n",
    "\n",
    "# 3. Remove duplicates\n",
    "def remove_duplicates(data):\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "clean_data = remove_duplicates(clean_data)\n",
    "print(\"After removing duplicates:\", clean_data.shape)\n",
    "\n",
    "# 4. Normalize numerical data\n",
    "def normalize_data(data, method='minmax'):\n",
    "    numeric_cols = data.select_dtypes(include='number').columns\n",
    "    scaler = MinMaxScaler() if method == 'minmax' else StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    return data\n",
    "\n",
    "clean_data = normalize_data(clean_data)\n",
    "print(\"After normalization:\")\n",
    "print(clean_data.head())\n",
    "\n",
    "# 5. Remove redundant features\n",
    "def remove_redundant_features(data, threshold=0.9):\n",
    "    corr_matrix = data.corr(numeric_only=True).abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return data.drop(columns=to_drop)\n",
    "\n",
    "clean_data = remove_redundant_features(clean_data)\n",
    "print(\"After removing redundant features:\", clean_data.shape)\n",
    "\n",
    "# 6. Train and evaluate model\n",
    "def simple_model(input_data, split_data=True, scale_data=False, print_report=True):\n",
    "    input_data = input_data.dropna()\n",
    "\n",
    "    # Assume the first column is the target\n",
    "    target = input_data.iloc[:, 0]\n",
    "    features = input_data.iloc[:, 1:]\n",
    "\n",
    "    # Encode categorical variables\n",
    "    for col in features.columns:\n",
    "        if features[col].dtype == 'object':\n",
    "            dummies = pd.get_dummies(features[col], prefix=col)\n",
    "            features = pd.concat([features.drop(columns=[col]), dummies], axis=1)\n",
    "\n",
    "    if split_data:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, target, test_size=0.2, stratify=target, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = features, features, target, target\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=100, solver='liblinear', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    if print_report:\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Run the model\n",
    "print(\"Training model...\")\n",
    "simple_model(clean_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
